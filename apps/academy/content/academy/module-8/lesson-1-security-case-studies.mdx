---
id: "m8-l1-security-cases"
title: "Security Case Studies & OWASP Top 10"
module: "Module 8 — Case Studies & Ethics"
xp: 180
durationMinutes: 40
isPremium: true
labPresets:
  - id: "lab-8-security-cases"
    preset: "Oracle - Secure Sandbox"
    initialPrompt: |
      Review this case study: A chatbot was manipulated into revealing private medical data. Identify the vulnerability and propose one defensive measure.
examples:
  - "Analyze a prompt injection case study where an LLM revealed system instructions."
  - "Study a jailbreak case where Developer Mode was exploited."
quiz:
  - q: "True or False: The OWASP Top 10 for LLMs provides a framework for identifying the most critical security risks."
    type: "tf"
    answer: true
  - q: "Which of these is NOT part of the OWASP Top 10 for LLMs?"
    type: "mcq"
    choices: ["Prompt injection","Data exfiltration","Jailbreaks","Token embedding optimization"]
    answer: "Token embedding optimization"
references:
  - title: "OWASP Top 10 for LLMs"
    url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
  - title: "PromptForge Compendium — Security Case Studies"
    url: "https://promptforgeai.internal/compendium#securitycases"
---

<LessonHeader title="Security Case Studies & OWASP Top 10" xp={180} duration={40} belt="Black" />

## Explainer

This lesson reviews real-world case studies of LLM security incidents and connects them to the **OWASP Top 10 for LLMs**. Learners will analyze scenarios involving prompt injection, jailbreaks, and exfiltration to understand vulnerabilities and defenses.

By studying failures, teams can better anticipate risks and build safer AI applications.

---

## Examples — Copy & Run

```text
# Example 1: Prompt injection
Analyze a prompt injection case study where an LLM revealed system instructions.

# Example 2: Jailbreak
Study a jailbreak case where Developer Mode was exploited.

# Example 3: Data leakage
Review this case study: A chatbot was manipulated into revealing private medical data. Identify the vulnerability and propose one defensive measure.
```

---

## Lab — Guided Practice

**Objective:** Learn from real-world failures to identify vulnerabilities.

1. Open **OraclePlayground** with preset `lab-8-security-cases`.
2. Run the medical chatbot exfiltration example.
3. Identify the vulnerability.
4. Propose at least one defense.

**Completion Criteria:** Learner connects case study to OWASP Top 10 categories and proposes a mitigation.

---

## Mini-Quest — Applied Challenge

**Task:** Select one OWASP Top 10 category (e.g., prompt injection). Create a case study scenario, then propose two defenses.

**Acceptance Criteria:**
- Case study is realistic.
- Defenses map to OWASP principles.
- Learner explains why defenses are effective.

---

## Quiz — Knowledge Check

1. **True / False:** The OWASP Top 10 for LLMs provides a framework for identifying the most critical security risks. → **True**
2. **MCQ:** Which of these is NOT part of the OWASP Top 10 for LLMs? → “Token embedding optimization.”

---

## References

- OWASP Top 10 for LLMs.  
- PromptForge Compendium — Security Case Studies.
